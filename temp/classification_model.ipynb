{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from an Excel file\n",
    "df = pd.read_excel(\"Grade_CS_Students.xlsx\", na_values=['NA'])\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display descriptive statistics for the DataFrame\n",
    "print(\"Descriptive statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of null values in each column\n",
    "print(\"Number of null values in each column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #of Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Non-Numeric Values\n",
    "# Create a dictionary to store the count of non-numeric values\n",
    "non_numeric_counts = {}\n",
    "\n",
    "# Loop through each column to count non-numeric values\n",
    "for col in df.columns:\n",
    "    # Convert to numeric, set non-numeric to NaN\n",
    "    non_numeric_col = pd.to_numeric(df[col], errors='coerce')\n",
    "    # Count NaN values (which represent non-numeric values)\n",
    "    non_numeric_counts[col] = non_numeric_col.isna().sum()\n",
    "\n",
    "# Convert the dictionary to a DataFrame for better readability\n",
    "non_numeric_counts_df = pd.DataFrame(list(non_numeric_counts.items()), columns=['Column', 'Non-Numeric Count'])\n",
    "\n",
    "# Display the counts\n",
    "print(non_numeric_counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop un necessary colms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "df = df.drop(['Year of enrolment', 'ID'], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### divide dataset to features & targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide features and targets\n",
    "target_subjects = ['CS501','CS502','CS503','CS504','CS505','CS506','CS507','CS508','CS509','CS510','CS512','CS597','CS598','MM507'] \n",
    "features = df.drop(target_subjects, axis=1)\n",
    "targets = df[target_subjects]\n",
    "\n",
    "# temp\n",
    "target = df['CS501']\n",
    "\n",
    "print(\"features : \", features.shape, \"\\ntargets : \", target.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### distribution of each feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the box plot for each column in features\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.boxplot(data=features)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### replace NULLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace null values\n",
    "\n",
    "for col in features.columns:\n",
    "    features[col] = pd.to_numeric(features[col], errors='coerce')  # Convert to numeric, set non-numeric to NaN\n",
    "    features.fillna({col: features[col].median()}, inplace=True)  # Fill NaN with median of the column\n",
    "\n",
    "# Verify the changes\n",
    "features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encode values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_grade(marks):\n",
    "\n",
    "# Convert scores into grades\n",
    "\n",
    "    if marks > 85:\n",
    "        return 0  #'A+'\n",
    "    elif 80 <= marks <= 85:\n",
    "        return 1  #'A'\n",
    "    elif 75 <= marks < 80:\n",
    "        return 2  #'A-'\n",
    "    elif 70 <= marks < 75:\n",
    "        return 3  #'B+'\n",
    "    elif 65 <= marks < 70:\n",
    "        return 4  #'B'\n",
    "    elif 60 <= marks < 65:\n",
    "        return 5  #'B-'\n",
    "    elif 55 <= marks < 60:\n",
    "        return 6  #'C+'\n",
    "    elif 50 <= marks < 55:\n",
    "        return 7  #'C'\n",
    "    elif 45 <= marks < 50:\n",
    "        return 8  #'C-'\n",
    "    elif 40 <= marks < 45:\n",
    "        return 9 #'D+'\n",
    "    elif 35 <= marks < 40:\n",
    "        return 10 #'D'\n",
    "    else:\n",
    "        return 11 #'E'\n",
    "\n",
    "\n",
    "# Apply the grade encoding function to each cell in the dataframe\n",
    "features_encoded = features.map(encode_grade)\n",
    "\n",
    "\n",
    "features_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode targets\n",
    "targets_encoded = targets.map(encode_grade)\n",
    "\n",
    "targets_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation matrix for encoded features use blue for negative correlation and red for positive correlation\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(features_encoded.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### correlation between features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store correlation data for each target\n",
    "correlations = {}\n",
    "\n",
    "# Iterate over each column in targets_encoded\n",
    "for column in targets_encoded.columns:\n",
    "    correlation = features_encoded.corrwith(targets_encoded[column])\n",
    "    correlation.sort_values(inplace=True)\n",
    "    correlations[column] = correlation\n",
    "\n",
    "    # Plot the correlation between features and the current target\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    correlation.plot(kind='bar', color='blue')\n",
    "    plt.title(f'Correlation between features and target: {column}')\n",
    "    plt.show()\n",
    "\n",
    "# Plot min and max correlations for each target\n",
    "min_correlations = {target: corr.min() for target, corr in correlations.items()}\n",
    "max_correlations = {target: corr.max() for target, corr in correlations.items()}\n",
    "\n",
    "# Convert to DataFrame for plotting\n",
    "min_corr_df = pd.DataFrame(list(min_correlations.items()), columns=['Target', 'Min Correlation'])\n",
    "max_corr_df = pd.DataFrame(list(max_correlations.items()), columns=['Target', 'Max Correlation'])\n",
    "\n",
    "# Plot min correlations\n",
    "plt.figure(figsize=(10, 5))\n",
    "min_corr_df.set_index('Target')['Min Correlation'].plot(kind='bar', color='red')\n",
    "plt.title('Minimum Correlation for Each Target')\n",
    "plt.show()\n",
    "\n",
    "# Plot max correlations\n",
    "plt.figure(figsize=(10, 5))\n",
    "max_corr_df.set_index('Target')['Max Correlation'].plot(kind='bar', color='green')\n",
    "plt.title('Maximum Correlation for Each Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### feature(encoded) distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot subplot bar charts for each feature\n",
    "plt.figure(figsize=(40, 30))\n",
    "for i, col in enumerate(features.columns):\n",
    "    plt.subplot(4, 13, i + 1)\n",
    "    plt.bar(features_encoded[col].value_counts().index, features_encoded[col].value_counts().values)\n",
    "    plt.title(col)\n",
    "    plt.xlabel('Grade')\n",
    "    plt.ylabel('Count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### balancing data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each column in targets_encoded\n",
    "for column in targets_encoded.columns:\n",
    "    value_counts = targets_encoded[column].value_counts()\n",
    "    \n",
    "    # Plot the value counts as a bar chart\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    value_counts.plot(kind='bar', color='blue')\n",
    "    plt.title(f'Value Counts for Target: {column}')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the value counts\n",
    "    print(f'Value Counts for Target: {column}')\n",
    "    print(value_counts)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combine smaller classes to form classes with more than 4 samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to merge small classes into the next larger class\n",
    "def merge_small_classes(target_column, threshold=4):\n",
    "    value_counts = target_column.value_counts()\n",
    "    small_classes = value_counts[value_counts < threshold].index\n",
    "    for small_class in small_classes:\n",
    "        # Find the next larger class\n",
    "        larger_classes = value_counts[value_counts >= threshold].index\n",
    "        if len(larger_classes) > 0:\n",
    "            next_larger_class = larger_classes[0]\n",
    "            target_column[target_column == small_class] = next_larger_class\n",
    "        else:\n",
    "            # If no larger class exists, keep the class as is\n",
    "            continue\n",
    "    return target_column\n",
    "\n",
    "# Iterate over each column in targets_encoded\n",
    "for column in targets_encoded.columns:\n",
    "    # Merge small classes in the target column\n",
    "    targets_encoded[column] = merge_small_classes(targets_encoded[column])\n",
    "    \n",
    "    # Print the value counts after merging small classes\n",
    "    value_counts = targets_encoded[column].value_counts()\n",
    "    print(f'Value Counts for Target (after merging small classes): {column}')\n",
    "    print(value_counts)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply smote Oversampling to balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(features, target, k_neighbors=3):\n",
    "    smote = SMOTE(k_neighbors=k_neighbors)\n",
    "    features_resampled, target_resampled = smote.fit_resample(features, target)\n",
    "    return features_resampled, target_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to each target column\n",
    "features_resampled_dict = {}\n",
    "targets_resampled_dict = {}\n",
    "\n",
    "for column in targets_encoded.columns:\n",
    "    features_resampled, target_resampled = apply_smote(features_encoded, targets_encoded[column])\n",
    "    features_resampled_dict[column] = (features_resampled, target_resampled)\n",
    "    targets_resampled_dict[column] = target_resampled\n",
    "\n",
    "    # Display the resampled target value counts\n",
    "    print(f'Resampled target value counts for {column}:')\n",
    "    print(pd.Series(target_resampled).value_counts())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>TRAIN + PREDICT + EVALUATE</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_evaluate_for_each_target(features_resampled_dict, targets_resampled_dict):\n",
    "    results = {}\n",
    "\n",
    "    # Iterate over each resampled dataset\n",
    "    for column, (features_resampled, target_resampled) in features_resampled_dict.items():\n",
    "        \n",
    "\n",
    "        #----------------------------DEFINE TRAIN TEST SET-------------------------------------\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features_resampled, target_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "        ############################ change according to the model ############################\n",
    "        #----------------------------LABEL RE-ENCODE (OPTIONAL)--------------------------------\n",
    "        # Encode target labels if they are not numeric\n",
    "        le = LabelEncoder()\n",
    "        y_train = le.fit_transform(y_train)\n",
    "        y_test = le.transform(y_test)  # Use transform for consistency\n",
    "        \n",
    "        ############################ change according to the model ############################\n",
    "        #-----------------------------CREATE THE MODEL----------------------------------------- \n",
    "\n",
    "        model = XGBClassifier()\n",
    "        model_name = 'XG Boost'\n",
    "\n",
    "        ############################ change according to the model ############################\n",
    "        #-----------------------------TRAIN FOR TRAIN SET-------------------------------------- \n",
    "        # Train the classifier\n",
    "        print(f\"Training {model_name} for target: {column}\")\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        ############################ change according to the model ############################\n",
    "        #-----------------------------PREDICT FOR TEST SET------------------------------------- \n",
    "        # Predict the target values\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "\n",
    "        #-----------------------------EVALUATION-----------------------------------------------\n",
    "        result = {}\n",
    "\n",
    "        # Calculate the accuracy, precision, recall, and F1 score of the classifier\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        result['accuracy'] = accuracy\n",
    "        result['precision'] = precision\n",
    "        result['recall'] = recall\n",
    "        result['f1'] = f1\n",
    "\n",
    "        # Display the precision, recall, and F1 score\n",
    "        print(f'Accuracy: {accuracy}')\n",
    "        print(f'Precision: {precision}')\n",
    "        print(f'Recall: {recall}')\n",
    "        print(f'F1 Score: {f1}')\n",
    "\n",
    "        # add result for each target\n",
    "        results[column] = result\n",
    "\n",
    "        # Calculate the confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Display the confusion matrix\n",
    "        print('Confusion Matrix:')\n",
    "        print(conf_matrix)\n",
    "        print()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for each resampled target\n",
    "\n",
    "\n",
    "results = train_model_and_evaluate_for_each_target(features_resampled_dict, targets_resampled_dict)\n",
    "print(\"Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for plotting\n",
    "subjects = list(results.keys())\n",
    "accuracies = [results[subject]['accuracy'] for subject in subjects]\n",
    "precisions = [results[subject]['precision'] for subject in subjects]\n",
    "recalls = [results[subject]['recall'] for subject in subjects]\n",
    "f1_scores = [results[subject]['f1'] for subject in subjects]\n",
    "\n",
    "# Plotting the results\n",
    "x = range(len(subjects))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "bar_width = 0.2\n",
    "\n",
    "bars1 = plt.bar(x, accuracies, bar_width, label='Accuracy')\n",
    "bars2 = plt.bar([i + bar_width for i in x], precisions, bar_width, label='Precision')\n",
    "bars3 = plt.bar([i + 2 * bar_width for i in x], recalls, bar_width, label='Recall')\n",
    "bars4 = plt.bar([i + 3 * bar_width for i in x], f1_scores, bar_width, label='F1 Score')\n",
    "\n",
    "# Add values on top of the bars\n",
    "def add_values(bars):\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2.0, yval, f'{yval:.2f}', ha='center', va='bottom', rotation=90)\n",
    "\n",
    "add_values(bars1)\n",
    "add_values(bars2)\n",
    "add_values(bars3)\n",
    "add_values(bars4)\n",
    "\n",
    "plt.xlabel('Subjects')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Evaluation Metrics(For Test Set) for Each Subject')\n",
    "plt.xticks([i + 1.5 * bar_width for i in x], subjects)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
